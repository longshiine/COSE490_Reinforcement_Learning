{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Environment (3 states walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm_notebook as tqdm  # you may need to install tqdm by \"pip install tqdm\"\n",
    "from itertools import cycle, count\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These codes are developed by Miguel Morales\n",
    "# Visit: https://github.com/mimoralea/gdrl\n",
    "\n",
    "def print_policy(pi, P, action_symbols=('<', 'v', '>', '^'), n_cols=4, title='Policy:'):\n",
    "    print(title)\n",
    "    arrs = {k:v for k,v in enumerate(action_symbols)}\n",
    "    for s in range(len(P)):\n",
    "        a = pi(s)\n",
    "        print(\"| \", end=\"\")\n",
    "        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n",
    "            print(\"\".rjust(9), end=\" \")\n",
    "        else:\n",
    "            print(str(s).zfill(2), arrs[a].rjust(6), end=\" \")\n",
    "        if (s + 1) % n_cols == 0: print(\"|\")\n",
    "    return\n",
    "\n",
    "def print_state_value_function(V, P, n_cols=4, prec=3, title='State-value function:'):\n",
    "    print(title)\n",
    "    for s in range(len(P)):\n",
    "        v = V[s]\n",
    "        print(\"| \", end=\"\")\n",
    "        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n",
    "            print(\"\".rjust(9), end=\" \")\n",
    "        else:\n",
    "            print(str(s).zfill(2), '{}'.format(np.round(v, prec)).rjust(6), end=\" \")\n",
    "        if (s + 1) % n_cols == 0: print(\"|\")\n",
    "    return\n",
    "\n",
    "def print_action_value_function(Q, \n",
    "                                optimal_Q=None, \n",
    "                                action_symbols=('<', '>'), \n",
    "                                prec=3, \n",
    "                                title='Action-value function:'):\n",
    "    vf_types=('',) if optimal_Q is None else ('', '*', 'err')\n",
    "    headers = ['s',] + [' '.join(i) for i in list(itertools.product(vf_types, action_symbols))]\n",
    "    print(title)\n",
    "    states = np.arange(len(Q))[..., np.newaxis]\n",
    "    arr = np.hstack((states, np.round(Q, prec)))\n",
    "    if not (optimal_Q is None):\n",
    "        arr = np.hstack((arr, np.round(optimal_Q, prec), np.round(optimal_Q-Q, prec)))\n",
    "    print(tabulate(arr, headers, tablefmt=\"fancy_grid\"))\n",
    "    return\n",
    "\n",
    "def probability_success(env, pi, goal_state, n_episodes=100, max_steps=200):\n",
    "    random.seed(123); np.random.seed(123) ; env.seed(123)\n",
    "    results = []\n",
    "    for _ in range(n_episodes):\n",
    "        state, done, steps = env.reset(), False, 0\n",
    "        while not done and steps < max_steps:\n",
    "            state, _, done, h = env.step(pi(state))\n",
    "            steps += 1\n",
    "        results.append(state == goal_state)\n",
    "    return np.sum(results)/len(results)\n",
    "\n",
    "def mean_return(env, pi, n_episodes=100, max_steps=200):\n",
    "    random.seed(123); np.random.seed(123) ; env.seed(123)\n",
    "    results = []\n",
    "    for _ in range(n_episodes):\n",
    "        state, done, steps = env.reset(), False, 0\n",
    "        results.append(0.0)\n",
    "        while not done and steps < max_steps:\n",
    "            state, reward, done, _ = env.step(pi(state))\n",
    "            results[-1] += reward\n",
    "            steps += 1\n",
    "    return np.mean(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import FrozenLake-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "#P = env.env.P\n",
    "init_state = env.reset()\n",
    "goal_state = 15\n",
    "\n",
    "LEFT, DOWN, RIGHT, UP = range(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Reward for each (s,a)\n------------------------------------------------------------------\n     -100.0          -100.0          -100.0          -100.0     \n -100.0   0.0     0.0     0.0     0.0     0.0     0.0    -100.0 \n      0.0            -10.0            0.0            -10.0      \n------------------------------------------------------------------\n      0.0             0.0             0.0             0.0       \n -100.0  -10.0    0.0     0.0    -10.0   -10.0    0.0    -100.0 \n      0.0             0.0             -5.0            0.0       \n------------------------------------------------------------------\n      0.0            -10.0            0.0            -10.0      \n -100.0   0.0     0.0     -5.0    0.0     0.0     -5.0   -100.0 \n      -5.0            0.0             0.0             1.0       \n------------------------------------------------------------------\n      0.0             0.0             -5.0             0        \n -100.0   0.0     -5.0    0.0     0.0     1.0      0       0    \n     -100.0          -100.0          -100.0            0        \n------------------------------------------------------------------\n"
    }
   ],
   "source": [
    "# This code generates the MDP kernel \n",
    "# we consider a deterministic environment (i.e., transition probability = 1.0)\n",
    "# action(0(L), 1(D), 2(R), 3(U)): [(prob, next state, reward, done)]\n",
    "\n",
    "detP = {}\n",
    "for i in range(16):\n",
    "    a = {}\n",
    "    # 0\n",
    "    if i % 4 == 0: # left wall\n",
    "        a[0] = [(1, -1, -100.0, True)]\n",
    "    else:\n",
    "        # check penalty or terminal state\n",
    "        if i - 1 in [5, 7]:\n",
    "            a[0] = [(1, i - 1, -10.0, False)]\n",
    "        elif i - 1 in [10, 12]:\n",
    "            a[0] = [(1, i - 1, -5.0, False)]\n",
    "        elif i - 1 == 15:\n",
    "            a[0] = [(1, i - 1, 1.0, True)]\n",
    "        # plain state\n",
    "        else:\n",
    "            a[0] = [(1, i - 1, 0.0, False)]\n",
    "            \n",
    "    # 1\n",
    "    if i + 4 > 15: # bottom wall\n",
    "        a[1] = [(1, -1, -100.0, True)]\n",
    "    else:\n",
    "        if i + 4 in [5, 7]:\n",
    "            a[1] = [(1, i + 4, -10.0, False)]\n",
    "        elif i + 4 in [10, 12]:\n",
    "            a[1] = [(1, i + 4, -5.0, False)]\n",
    "        elif i + 4 == 15:\n",
    "            a[1] = [(1, i + 4, 1.0, True)]\n",
    "        else:\n",
    "            a[1] = [(1, i + 4, 0.0, False)]\n",
    "            \n",
    "    # 2\n",
    "    if i % 4 == 3: # right wall\n",
    "        a[2] = [(1, -1, -100.0, True)]\n",
    "    else:\n",
    "        if i + 1 in [5, 7]:\n",
    "            a[2] = [(1, i + 1, -10.0, False)]\n",
    "        elif i + 1 in [10, 12]:\n",
    "            a[2] = [(1, i + 1, -5.0, False)]\n",
    "        elif i + 1 == 15:\n",
    "            a[2] = [(1, i + 1, 1.0, True)]\n",
    "        else:\n",
    "            a[2] = [(1, i + 1, 0.0, False)]\n",
    "            \n",
    "    # 3\n",
    "    if i - 4 < 0: # upper wall\n",
    "        a[3] = [(1, -1, -100.0, True)]\n",
    "    else:\n",
    "        if i - 4 in [5, 7]:\n",
    "            a[3] = [(1, i - 4, -10.0, False)]\n",
    "        elif i - 4 in [10, 12]:\n",
    "            a[3] = [(1, i - 4, -5.0, False)]\n",
    "        elif i - 4 == 15:\n",
    "            a[3] = [(1, i - 4, 1.0, True)]\n",
    "        else:\n",
    "            a[3] = [(1, i - 4, 0.0, False)]\n",
    "\n",
    "    if i == 15:\n",
    "        a = {\n",
    "            0: [(1.0, 15, 0, True)],\n",
    "            1: [(1.0, 15, 0, True)],\n",
    "            2: [(1.0, 15, 0, True)],\n",
    "            3: [(1.0, 15, 0, True)]}\n",
    "        \n",
    "    detP[i] = a\n",
    "    \n",
    "P = detP\n",
    "env.env.P = P\n",
    "#P\n",
    "\n",
    "def print_R (P):\n",
    "    sv_up, sv_middle, sv_down = \"\",\"\",\"\"\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    for s in range(16):\n",
    "        _,_,v_up,_ = P[s][UP][0]\n",
    "        _,_,v_left,_ = P[s][LEFT][0] \n",
    "        _,_,v_right,_ = P[s][RIGHT][0]\n",
    "        _,_,v_down,_ = P[s][DOWN][0]\n",
    "        sv_up = sv_up + str(v_up).center(16, \" \")\n",
    "        sv_middle = sv_middle + str(v_left).center(8, \" \") + str(v_right).center(8, \" \")\n",
    "        sv_down = sv_down + str(v_down).center(16,\" \")\n",
    "        if( (s+1) % 4 == 0):\n",
    "            print(sv_up)\n",
    "            print(sv_middle)\n",
    "            print(sv_down)\n",
    "            print(\"------------------------------------------------------------------\")\n",
    "            sv_up, sv_middle, sv_down = \"\",\"\",\"\"\n",
    "\n",
    "print(\"Reward for each (s,a)\")\n",
    "print_R(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Policy:\n| 00      > | 01      > | 02      > | 03      v |\n| 04      v | 05      v | 06      > | 07      v |\n| 08      v | 09      v | 10      v | 11      v |\n| 12      > | 13      > | 14      > |           |\nReaches goal 100.00%. Obtains an average undiscounted return of -9.0000.\n"
    }
   ],
   "source": [
    "LEFT, DOWN, RIGHT, UP = range(4)\n",
    "\n",
    "# base policy\n",
    "base_pi = lambda s: {\n",
    "    0:RIGHT, 1:RIGHT, 2:RIGHT, 3:DOWN,\n",
    "    4:DOWN, 5:DOWN, 6:RIGHT, 7:DOWN,\n",
    "    8:DOWN, 9:DOWN, 10:DOWN, 11:DOWN,\n",
    "    12:RIGHT, 13:RIGHT, 14:RIGHT, 15:LEFT\n",
    "}[s]\n",
    "\n",
    "print_policy(base_pi, P)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, base_pi, goal_state=goal_state)*100, \n",
    "    mean_return(env, base_pi)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation, policy improvement, policy iteration, value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "State-value function:\n| 00 -8.752 | 01 -8.8404 | 02 -8.9297 | 03 -9.0199 |\n| 04 -3.9894 | 05 0.9703 | 06 -9.0199 | 07   0.99 |\n| 08 -4.0297 | 09 0.9801 | 10   0.99 | 11    1.0 |\n| 12 0.9801 | 13   0.99 | 14    1.0 |           |\n"
    }
   ],
   "source": [
    "def policy_evaluation(pi, P, gamma=1.0, theta=1e-10):\n",
    "    prev_V = np.zeros(len(P), dtype=np.float64)\n",
    "    while True:\n",
    "        V = np.zeros(len(P), dtype=np.float64)\n",
    "        for s in range(len(P)):\n",
    "            for prob, next_state, reward, done in P[s][pi(s)]:\n",
    "                V[s] += prob * (reward + gamma * prev_V[next_state] * (not done))\n",
    "        if np.max(np.abs(prev_V - V)) < theta:\n",
    "            break\n",
    "        prev_V = V.copy()\n",
    "    return V\n",
    "\n",
    "def policy_improvement(V, P, gamma=1.0):\n",
    "    Q = np.zeros((len(P), len(P[0])), dtype=np.float64)\n",
    "    for s in range(len(P)):\n",
    "        for a in range(len(P[s])):\n",
    "            for prob, next_state, reward, done in P[s][a]:\n",
    "                Q[s][a] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "    new_pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    return new_pi\n",
    "\n",
    "def policy_iteration(P, gamma=1.0, theta=1e-10):\n",
    "    random_actions = np.random.choice(tuple(P[0].keys()), len(P))\n",
    "    pi = lambda s: {s:a for s, a in enumerate(random_actions)}[s]\n",
    "    while True:\n",
    "        old_pi = {s:pi(s) for s in range(len(P))}\n",
    "        V = policy_evaluation(pi, P, gamma, theta)\n",
    "        pi = policy_improvement(V, P, gamma)\n",
    "        if old_pi == {s:pi(s) for s in range(len(P))}:\n",
    "            break\n",
    "    return V, pi\n",
    "\n",
    "def value_iteration(P, gamma=1.0, theta=1e-10):\n",
    "    V = np.zeros(len(P), dtype=np.float64)\n",
    "    while True:\n",
    "        Q = np.zeros((len(P), len(P[0])), dtype=np.float64)\n",
    "        for s in range(len(P)):\n",
    "            for a in range(len(P[s])):\n",
    "                for prob, next_state, reward, done in P[s][a]:\n",
    "                    Q[s][a] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "        if np.max(np.abs(V - np.max(Q, axis=1))) < theta:\n",
    "            break\n",
    "        V = np.max(Q, axis=1)\n",
    "    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    return V, pi\n",
    "\n",
    "# example with go_get_pi\n",
    "V = policy_evaluation(base_pi, P, gamma=0.99)\n",
    "print_state_value_function(V, P, prec=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout Algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "# 0 : (s a s')=( 0 1 4 ), r = 0.0\n# 1 : (s a s')=( 4 1 8 ), r = 0.0\n# 2 : (s a s')=( 8 2 9 ), r = 0.0\n# 3 : (s a s')=( 9 1 13 ), r = 0.0\n# 4 : (s a s')=( 13 2 14 ), r = 0.0\n# 5 : (s a s')=( 14 2 15 ), r = 1.0\n"
    }
   ],
   "source": [
    "def do_rollout(current_pi, base_pi, env, current_state, gamma=1.0, max_steps=100):\n",
    "    \n",
    "    # possible_actions (Left:0, Down:1, Right:2, Up:3)\n",
    "    possible_actions = [action for action in range(4)]\n",
    "    \n",
    "    # Q tilda list\n",
    "    Q_list = np.zeros((len(possible_actions)), dtype=np.float64)\n",
    "    \n",
    "    # Simulation for every possible u_k\n",
    "    for idx, a in enumerate(possible_actions):\n",
    "        state, done = env.reset(), False\n",
    "        \n",
    "        # env.step to current_state using current_pi\n",
    "        while state != current_state:\n",
    "            state, _, _, _ = env.step(current_pi(state))\n",
    "            \n",
    "        # one step from current state by action u_ks\n",
    "        next_state, reward, done, _ = env.step(a) # [(prob, next state, reward, done)]\n",
    "        if done: # if after u_k => done\n",
    "            Q_list[idx] = reward\n",
    "            continue\n",
    "        \n",
    "        # Heuristic: base_policy\n",
    "        steps = 0\n",
    "        reward_sum = reward\n",
    "        \n",
    "        H_next_state, H_reward, H_done, info = env.step(base_pi(next_state)) # after base_policy one step\n",
    "        while True:\n",
    "            steps += 1\n",
    "            if steps > max_steps or H_done:\n",
    "                break\n",
    "            reward_sum += pow(gamma, steps) * H_reward\n",
    "            H_next_state, H_reward, H_done, info = env.step(base_pi(H_next_state))\n",
    "        Q_list[idx] = reward_sum\n",
    "        \n",
    "    return np.argmax(Q_list)\n",
    "    \n",
    "def online_rollout(env, base_pi, sim_env):\n",
    "    nS = env.observation_space.n\n",
    "    pi = {state:base_pi(state) for state in range(nS)}    \n",
    "\n",
    "    ncount = 0\n",
    "    reward = 0\n",
    "    state, done = env.reset(), False\n",
    "\n",
    "    while not done:\n",
    "        current_pi = lambda s: {s:pi[s] for s in range(nS)}[s]\n",
    "        pi[state] = do_rollout(current_pi, base_pi, sim_env, state, gamma=0.99, max_steps=100)\n",
    "        next_state, reward, done, _ = env.step(pi[state])\n",
    "\n",
    "        print(\"#\", ncount, \": (s a s')=(\", state, pi[state], next_state, \"), r =\", reward)\n",
    "        ncount = ncount+1\n",
    "        state = next_state\n",
    "        \n",
    "    return_pi = lambda s: {s:pi[s] for s in range(nS)}[s]\n",
    "    return return_pi\n",
    "\n",
    "sim_env = gym.make('FrozenLake-v0')  # Need to run the base policy over this simulation environment\n",
    "sim_env.env.P = P\n",
    "rollout_pi = online_rollout(env, base_pi, sim_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Base policy\nPolicy:\n| 00      > | 01      > | 02      > | 03      v |\n| 04      v | 05      v | 06      > | 07      v |\n| 08      v | 09      v | 10      v | 11      v |\n| 12      > | 13      > | 14      > |           |\nState-value function:\n| 00 -8.752 | 01 -8.8404 | 02 -8.9297 | 03 -9.0199 |\n| 04 -3.9894 | 05 0.9703 | 06 -9.0199 | 07   0.99 |\n| 08 -4.0297 | 09 0.9801 | 10   0.99 | 11    1.0 |\n| 12 0.9801 | 13   0.99 | 14    1.0 |           |\n\nRollout policy\nPolicy:\n| 00      v | 01      > | 02      > | 03      v |\n| 04      v | 05      v | 06      > | 07      v |\n| 08      > | 09      v | 10      v | 11      v |\n| 12      > | 13      > | 14      > |           |\nState-value function:\n| 00  0.951 | 01 -8.8404 | 02 -8.9297 | 03 -9.0199 |\n| 04 0.9606 | 05 0.9703 | 06 -9.0199 | 07   0.99 |\n| 08 0.9703 | 09 0.9801 | 10   0.99 | 11    1.0 |\n| 12 0.9801 | 13   0.99 | 14    1.0 |           |\n"
    }
   ],
   "source": [
    "print(\"Base policy\")\n",
    "print_policy(base_pi, P)\n",
    "V = policy_evaluation(base_pi, P, gamma=0.99)\n",
    "print_state_value_function(V, P, prec=4)\n",
    "\n",
    "print()\n",
    "print(\"Rollout policy\")\n",
    "print_policy(rollout_pi, P)\n",
    "V = policy_evaluation(rollout_pi, P, gamma=0.99)\n",
    "print_state_value_function(V, P, prec=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}